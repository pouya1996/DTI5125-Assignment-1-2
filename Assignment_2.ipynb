{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJd0wRqyThnhNExMupkX47"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gutenberg Text Preprocessing\n",
        "\n",
        "This script demonstrates how to:\n",
        "- Download and use NLTK resources (stopwords, punkt, wordnet, gutenberg).\n",
        "- Clean text using regex, remove stopwords, lemmatize, and stem tokens.\n",
        "- Randomly partition cleaned tokens into equal-sized chunks.\n",
        "- Organize data into a Pandas DataFrame for further use or modeling.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.x  \n",
        "- NLTK  \n",
        "- pandas  \n",
        "- (Optional) Jupyter or similar environment for interactive exploration\n",
        "\n",
        "## Installation\n",
        "\n",
        "1. Install dependencies:\n",
        "    ```bash\n",
        "    pip install nltk pandas\n",
        "    ```\n",
        "2. Download necessary NLTK data (stopwords, punkt, wordnet, gutenberg):\n",
        "    ```python\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('gutenberg')\n",
        "    ```\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. **Set your file IDs** and corresponding labels:\n",
        "    ```python\n",
        "    fileids = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']\n",
        "    labels = ['a', 'b', 'c']\n",
        "    ```\n",
        "2. **Call the main function** to generate a DataFrame of cleaned text partitions:\n",
        "    ```python\n",
        "    df = process_gutenberg_books(fileids, labels, n_partitions=200, partition_size=100)\n",
        "    ```\n",
        "3. **Export or examine** the resulting DataFrame:\n",
        "    ```python\n",
        "    df.to_csv('gutenberg_cleaned_partitions.csv', index=False)\n",
        "    print(df.head())\n",
        "    ```\n",
        "\n",
        "## Functions Overview\n",
        "\n",
        "1. **`clean_text_words(word_list)`**  \n",
        "   Cleans and normalizes a list of tokens (lowercase, remove non-alpha, remove stopwords, lemmatize, stem).\n",
        "\n",
        "2. **`get_random_partitions_of_tokens(cleaned_tokens, label, n_partitions=200, partition_size=100)`**  \n",
        "   Creates random slices of cleaned tokens and pairs them with a label.\n",
        "\n",
        "3. **`process_gutenberg_books(fileids, labels, n_partitions=200, partition_size=100)`**  \n",
        "   - Retrieves raw text from the Gutenberg corpus.  \n",
        "   - Tokenizes and cleans the text.  \n",
        "   - Partitions cleaned tokens into multiple labeled subsets.  \n",
        "   - Returns a Pandas DataFrame of (label, text) rows.\n",
        "\n",
        "## Example\n",
        "\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    fileids = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']\n",
        "    labels = ['a', 'b', 'c']\n",
        "    df = process_gutenberg_books(fileids, labels)\n",
        "    df.to_csv('gutenberg_cleaned_partitions.csv', index=False)\n",
        "    print(df.head())\n"
      ],
      "metadata": {
        "id": "k1qkkca4NEPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# NLTK downloads (stopwords, punkt, wordnet) -- run once if you haven't\n",
        "# -------------------------------------------------------------------\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import stopwords, gutenberg\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text_words(word_list):\n",
        "    \"\"\"\n",
        "    Perform text cleaning steps:\n",
        "      - Lowercase\n",
        "      - Remove punctuation/numbers (regex)\n",
        "      - Remove stopwords\n",
        "      - Lemmatize\n",
        "      - Stem (optional)\n",
        "    Return the cleaned list of tokens.\n",
        "    \"\"\"\n",
        "    cleaned_words = []\n",
        "    for w in word_list:\n",
        "        # Lowercase\n",
        "        w = w.lower()\n",
        "        # Keep only alphabetic characters\n",
        "        w = re.sub(r'[^a-z]', '', w)\n",
        "\n",
        "        if w and w not in stop_words:\n",
        "            # Lemmatize\n",
        "            w_lemma = lemmatizer.lemmatize(w)\n",
        "            # Stem\n",
        "            w_stem = stemmer.stem(w_lemma)\n",
        "            cleaned_words.append(w_stem)\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "def get_random_partitions_of_tokens(cleaned_tokens, label, n_partitions=200, partition_size=100):\n",
        "    \"\"\"\n",
        "    Given a list of cleaned tokens (already preprocessed) and a label:\n",
        "      - Randomly select 'n_partitions' slices of length 'partition_size'\n",
        "      - Return a list of tuples: (label, [list_of_cleaned_tokens_for_partition])\n",
        "    \"\"\"\n",
        "    partitions = []\n",
        "    max_start_index = len(cleaned_tokens) - partition_size\n",
        "\n",
        "    # If not enough tokens for a single partition of size 'partition_size', return empty\n",
        "    if max_start_index < 0:\n",
        "        return partitions\n",
        "\n",
        "    for _ in range(n_partitions):\n",
        "        start = random.randint(0, max_start_index)\n",
        "        chunk = cleaned_tokens[start : start + partition_size]\n",
        "        partitions.append((label, chunk))\n",
        "\n",
        "    return partitions\n",
        "\n",
        "def process_gutenberg_books(fileids, labels, n_partitions=200, partition_size=100):\n",
        "    \"\"\"\n",
        "    Takes:\n",
        "      - fileids: list of fileids from nltk.corpus.gutenberg (e.g., ['austen-emma.txt', ...])\n",
        "      - labels: list of labels (same length as fileids)\n",
        "    Returns a Pandas DataFrame with columns [label, text],\n",
        "    where each row has exactly 100 cleaned words in the 'text'.\n",
        "    \"\"\"\n",
        "    all_partitions = []\n",
        "\n",
        "    for fileid, label in zip(fileids, labels):\n",
        "        # 1) Load raw text from Gutenberg\n",
        "        text = gutenberg.raw(fileid)\n",
        "\n",
        "        # 2) Tokenize\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "\n",
        "        # 3) Clean the tokens (stopwords, lemmatization, stemming, etc.)\n",
        "        cleaned_tokens = clean_text_words(tokens)\n",
        "\n",
        "        # 4) Get random partitions of size `partition_size` from cleaned tokens\n",
        "        partitions = get_random_partitions_of_tokens(\n",
        "            cleaned_tokens,\n",
        "            label,\n",
        "            n_partitions=n_partitions,\n",
        "            partition_size=partition_size\n",
        "        )\n",
        "\n",
        "        # 5) Convert each list of cleaned tokens to a single string\n",
        "        for lbl, chunk_words in partitions:\n",
        "            combined_text = \" \".join(chunk_words)\n",
        "            all_partitions.append((lbl, combined_text))\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_partitions, columns=['label', 'text'])\n",
        "    return df\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Example usage\n",
        "# -------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # For demonstration, let's pick three Gutenberg file IDs\n",
        "    fileids = ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt']\n",
        "    labels = ['a', 'b', 'c']  # one label per file\n",
        "\n",
        "    # Generate the DataFrame\n",
        "    df = process_gutenberg_books(fileids, labels, n_partitions=200, partition_size=100)\n",
        "\n",
        "    # Save to CSV\n",
        "    df.to_csv('gutenberg_cleaned_partitions.csv', index=False)\n",
        "\n",
        "    # Check some rows\n",
        "    print(\"Data preparation complete. Sample rows:\")\n",
        "    print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNrp5K59KSk",
        "outputId": "dd467d4e-3a2e-42ef-bd62-fd98d5fd1cdb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete. Sample rows:\n",
            "  label                                               text\n",
            "0     a  face suit give belief produc anoth dread perha...\n",
            "1     a  swell half hour relat contain multipli proof s...\n",
            "2     a  mr knightley must never marri littl henri must...\n",
            "3     a  like charad slight much better passion poet lo...\n",
            "4     a  would wish leg salt know nice loin dress direc...\n"
          ]
        }
      ]
    }
  ]
}